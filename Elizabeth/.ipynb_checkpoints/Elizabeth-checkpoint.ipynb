{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***QTM 350 Final Project***\n",
    "Olivia Song, Elizabeth Bryant, Betty Li, and Aaron Leonard\n",
    "\n",
    "For our Data Science Capstone, our group analyzed NOAA Global Historical Climatology Network Daily data from the Registry of Open Data on AWS (https://registry.opendata.aws/noaa-ghcn/). This dataset contains daily observations for precipitation, temperature, snow depth, and time of observation from multiple resources since 1763. We decided to focus on data from 2019, since it is the most current and complete. Our work can be found at https://github.com/miaowu128/QTM350FinalProject.git.\n",
    "\n",
    "We began by looking at the documentation included on the Open Data Registry (https://docs.opendata.aws/noaa-ghcn-pds/readme.html). The readme file gave us insight on the variables we would be looking at. Dr. Jacobson also helped us research some other published notebooks that used the data to see if we could build off of previous work. These were great examples;however, the first notebook used a different file than the one we chose and the second was used to demonstrate how to use various python packages. \n",
    "\n",
    "### *Notebook Examples*\n",
    "- https://livebook.manning.com/book/the-quick-python-book-third-edition/case-study/1\n",
    "- https://www.kaggle.com/kerneler/starter-aws-open-source-weather-4e419ab3-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Data Architecture***\n",
    "Throughout this semester we have learned about many AWS services that can be useful for running big data. Some of the services we used for our project are: Registry of Open Data, s3, EC2 instance, and SageMaker. Using draw.io, we created a diagram to show our workflow.\n",
    "![Data Architecture](QTM350FinalProject_EB/QTM350FinalProject/Elizabeth/QTM350_FinalProject.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket):\n",
    "    import logging\n",
    "\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        logging.error(e)\n",
    "        return 'Bucket ' + bucket + ' could not be created.'\n",
    "    return 'Created or already exists ' + bucket + ' bucket.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Created or already exists open-data-analytics-noaa bucket.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bucket('open-data-analytics-noaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_buckets(match=''):\n",
    "    response = s3.list_buckets()\n",
    "    if match:\n",
    "        print(f'Existing buckets containing \"{match}\" string:')\n",
    "    else:\n",
    "        print('All existing buckets:')\n",
    "    for bucket in response['Buckets']:\n",
    "        if match:\n",
    "            if match in bucket[\"Name\"]:\n",
    "                print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets containing \"noaa\" string:\n",
      "  open-data-analytics-noaa\n"
     ]
    }
   ],
   "source": [
    "list_buckets(match='noaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket, match='', size_mb=0):\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "        elif list_check and key_size_mb <= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket_contents(bucket='noaa-ghcn-pds', match='.csv', size_mb= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_csv_dataset(bucket, key, rows=10):\n",
    "    data_source = {\n",
    "            'Bucket': bucket,\n",
    "            'Key': key\n",
    "        }\n",
    "    # Generate the URL to get Key from Bucket\n",
    "    url = s3.generate_presigned_url(\n",
    "        ClientMethod = 'get_object',\n",
    "        Params = data_source\n",
    "    )\n",
    "\n",
    "    data = pd.read_csv(url, nrows=rows, header = None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preview_csv_dataset(bucket='noaa-ghcn-pds', key='csv/2019.csv', rows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # can do group by (rename columns first)\n",
    "# Keep old code to tell story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_exists(bucket, key):\n",
    "    try:\n",
    "        s3_resource.Object(bucket, key).load()\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            # The key does not exist.\n",
    "            return(False)\n",
    "        else:\n",
    "            # Something else has gone wrong.\n",
    "            raise\n",
    "    else:\n",
    "        # The key does exist.\n",
    "        return(True)\n",
    "\n",
    "def copy_among_buckets(from_bucket, from_key, to_bucket, to_key):\n",
    "    if not key_exists(to_bucket, to_key):\n",
    "        s3_resource.meta.client.copy({'Bucket': from_bucket, 'Key': from_key}, \n",
    "                                        to_bucket, to_key)        \n",
    "        print(f'File {to_key} saved to S3 bucket {to_bucket}')\n",
    "    else:\n",
    "        print(f'File {to_key} already exists in S3 bucket {to_bucket}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_among_buckets(from_bucket='afsis', from_key='2009-2013/Wet_Chemistry/ICRAF/README.md',to_bucket=''open-data-analytics-afsis', to_key='ICRAF.README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
